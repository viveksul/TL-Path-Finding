{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build our project\n",
    "\n",
    "# Here we are building a mulitple input mixed data model that takes in the 3 inputs seperately. RBI, chaneel matrices remain \n",
    "# seperate but now instead of giving Spectrum Occupancy matrix as input we give only NSA and instead of Src Des matrix we \n",
    "# just give Src and Des node id. The NSA and Src_Des input are given in two different branches, in the next model they will \n",
    "# be given together on one branch. The output power are indices rather matrices. So its an Nx1 vector.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "# number of iterations the matlab code ran or the number of times the network is simulated\n",
    "itr = 50000\n",
    "# Number of iterations the test data generation matlab code ran or the number of times the network is simulated\n",
    "itr_test = 2000\n",
    "# number of nodes in the network\n",
    "N = 30\n",
    "# We scale the non zero values of output power so that there is some sizable error if the model outputs a zero matrix\n",
    "scaling_parameter = 1\n",
    "\n",
    "# Reading the RBI data\n",
    "df = pd.read_csv(\"Data for RBI TL 0.9 mean 50000_itr(N=30)/Data_rbi_vectors.csv\")\n",
    "#print(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_1 = df.to_numpy()\n",
    "#print(tem)\n",
    "X_1.shape\n",
    "\n",
    "#Just for the 1332 dataset as the last array will be nans\n",
    "#tem = tem[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94562fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=list(range(0, itr))\n",
    "# X_1 = np.zeros((itr,N))\n",
    "# for i in a:\n",
    "#     X_1[i:] = tem[i:]\n",
    "#X_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the channel data\n",
    "df1 = pd.read_csv(\"Data for RBI TL 0.9 mean 50000_itr(N=30)/Data_channel_matrices.csv\")\n",
    "#print(df1)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_2 = df1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5a868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = X_2.reshape((itr,N,N))\n",
    "#print(X_2)\n",
    "X_2 = np.nan_to_num(X_2)\n",
    "#X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6164f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Source_Destination_info data\n",
    "df2 = pd.read_csv(\"Data for RBI TL 0.9 mean 50000_itr(N=30)/Data_Source_Destination_info_matrix(only Src Des).csv\")\n",
    "#print(df2)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b680ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_3 = df2.to_numpy()\n",
    "#print(X_3)\n",
    "X_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = X_3.astype(np.float64)\n",
    "print(X_3.dtype)\n",
    "#print(X_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Spectrum_occupancy data, i.e, NSA\n",
    "df3 = pd.read_csv(\"Data for RBI TL 0.9 mean 50000_itr(N=30)/Data_spectrum_matrix.csv\")\n",
    "#print(df3)\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ada01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_4 = df3.to_numpy()\n",
    "#print(X_4)\n",
    "X_4.shape\n",
    "\n",
    "# Its already in float64 so no need to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf380eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the output power data\n",
    "df4 = pd.read_csv(\"Data for RBI TL 0.9 mean 50000_itr(N=30)/Data_output1.csv\")\n",
    "#print(df4)\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5598e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_5 = df4.to_numpy()\n",
    "#print(X_5)\n",
    "print(X_5.shape)\n",
    "X_5 = np.nan_to_num(X_5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88b452e7",
   "metadata": {},
   "source": [
    "## Testing Cell, just to work out code to convert the (1, 30) output to (1, 6)\n",
    "\n",
    "a=list(range(0, itr))\n",
    "max1 = 0\n",
    "count = 0\n",
    "max_path = np.zeros(30)\n",
    "max_path_ind = 0\n",
    "X_5_new = np.zeros(6)\n",
    "for i in a:\n",
    "    #print(\"Original\")\n",
    "    #print(X_5[i])\n",
    "    temp = X_5[i]\n",
    "    #print(\"Non zero elements\")\n",
    "    #print(temp[np.nonzero(temp)])\n",
    "    #print(\"Length of nonzero array\")\n",
    "    temp_ind = len(temp[np.nonzero(temp)])\n",
    "    #print(temp_ind)\n",
    "    if temp_ind > max1:\n",
    "        max1 = temp_ind\n",
    "        max_path = temp\n",
    "        max_path_ind = i\n",
    "        if max1 > 6:\n",
    "            count = count + 1\n",
    "            print(temp)\n",
    "            print(i)\n",
    "            \n",
    "print(\"max nonzero length\")\n",
    "print(max1)\n",
    "print(max_path)\n",
    "print(max_path_ind)\n",
    "print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5541c228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell converts each of the (1,30) output of y_data to (1,6)\n",
    "\n",
    "a=list(range(0, itr))\n",
    "max1 = 0\n",
    "count = 0\n",
    "maxlen = 0\n",
    "max_path = np.zeros(30)\n",
    "max_path_ind = 0\n",
    "X_5_new = np.zeros((itr,6))\n",
    "for i in a:\n",
    "    temp = X_5[i]\n",
    "    maxlen = len(temp[np.nonzero(temp)])\n",
    "    if maxlen > 6:\n",
    "        X_5_new[i,:] = temp[0:6]\n",
    "        temp_non_zero = temp[np.nonzero(temp)]\n",
    "        X_5_new[i,5] = temp_non_zero[-1]\n",
    "        continue\n",
    "    X_5_new[i,:] = temp[0:6]\n",
    "    #print(\"original\")\n",
    "    #print(X_5[i])\n",
    "X_5_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for 33569 itr data  the 150th and 7128 paths have length > 6 , ie, [ 6. 23. 27.  3. 15. 17. 18.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "#  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] and [30.  7.  8. 16.  3. 25. 12. 21.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "#  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e513a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = X_5_new.astype(np.float64)\n",
    "print(y_data.dtype)\n",
    "#print(y_data_temp)\n",
    "y_data.shape\n",
    "\n",
    "# No need to unravel it as its already a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1.dtype, X_2.dtype, X_3.dtype, X_4.dtype, y_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9292964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1.shape, X_2.shape, X_3.shape, X_4.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5a0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling parameter for y_data so model gets greater penalty for error and also since output is sparse\n",
    "\n",
    "y_data = y_data*scaling_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5784f93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y_data to tensor\n",
    "\n",
    "y_data = tf.constant(y_data)\n",
    "print(y_data.shape)\n",
    "#y_data = tf.reshape(y_data, [itr,1,1,N*N])\n",
    "#print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb0577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers.core import Activation\n",
    "from tensorflow.python.keras.layers.core import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import concatenate\n",
    "#from tensorflow.python.keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transfer Learning\n",
    "\n",
    "#Loading the pre-trained model trained on N=30\n",
    "base_model = keras.models.load_model('saved_model/TL_base_model(22-11-2023)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "print(\"Number of trainablr variables: \", len(base_model.trainable_variables))\n",
    "base_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 'base_model' to new variable 'model'\n",
    "model = base_model\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 23\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in model.layers[:fine_tune_at]:\n",
    "    layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88b82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the new model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "print(\"Number of trainablr variables: \", len(base_model.trainable_variables))\n",
    "base_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001), # SGD is short for stocastic gradient descent\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13611454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0200 - accuracy: 0.6972 - val_loss: 3.2874 - val_accuracy: 0.6660\n",
      "Epoch 58/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0195 - accuracy: 0.6959 - val_loss: 3.2950 - val_accuracy: 0.6573\n",
      "Epoch 59/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0192 - accuracy: 0.6955 - val_loss: 3.2975 - val_accuracy: 0.6560\n",
      "Epoch 60/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0170 - accuracy: 0.6964 - val_loss: 3.3029 - val_accuracy: 0.6493\n",
      "Epoch 61/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0130 - accuracy: 0.6963 - val_loss: 3.2909 - val_accuracy: 0.6667\n",
      "Epoch 62/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0156 - accuracy: 0.6965 - val_loss: 3.2842 - val_accuracy: 0.6640\n",
      "Epoch 63/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0124 - accuracy: 0.6980 - val_loss: 3.2764 - val_accuracy: 0.6700\n",
      "Epoch 64/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0158 - accuracy: 0.6969 - val_loss: 3.2835 - val_accuracy: 0.6647\n",
      "Epoch 65/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0129 - accuracy: 0.6966 - val_loss: 3.2846 - val_accuracy: 0.6720\n",
      "Epoch 66/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0131 - accuracy: 0.6964 - val_loss: 3.2863 - val_accuracy: 0.6600\n",
      "Epoch 67/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0099 - accuracy: 0.6981 - val_loss: 3.3203 - val_accuracy: 0.6440\n",
      "Epoch 68/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0124 - accuracy: 0.6968 - val_loss: 3.2926 - val_accuracy: 0.6593\n",
      "Epoch 69/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0110 - accuracy: 0.6979 - val_loss: 3.2902 - val_accuracy: 0.6587\n",
      "Epoch 70/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0101 - accuracy: 0.6967 - val_loss: 3.2817 - val_accuracy: 0.6653\n",
      "Epoch 71/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0087 - accuracy: 0.6968 - val_loss: 3.2831 - val_accuracy: 0.6633\n",
      "Epoch 72/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0070 - accuracy: 0.6978 - val_loss: 3.2952 - val_accuracy: 0.6627\n",
      "Epoch 73/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0070 - accuracy: 0.6967 - val_loss: 3.2787 - val_accuracy: 0.6620\n",
      "Epoch 74/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0071 - accuracy: 0.6971 - val_loss: 3.2948 - val_accuracy: 0.6593\n",
      "Epoch 75/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0073 - accuracy: 0.6968 - val_loss: 3.2994 - val_accuracy: 0.6613\n",
      "Epoch 76/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0060 - accuracy: 0.6967 - val_loss: 3.2793 - val_accuracy: 0.6660\n",
      "Epoch 77/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0085 - accuracy: 0.6987 - val_loss: 3.2922 - val_accuracy: 0.6660\n",
      "Epoch 78/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0079 - accuracy: 0.6980 - val_loss: 3.2848 - val_accuracy: 0.6693\n",
      "Epoch 79/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0056 - accuracy: 0.6979 - val_loss: 3.2854 - val_accuracy: 0.6633\n",
      "Epoch 80/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0064 - accuracy: 0.6985 - val_loss: 3.2986 - val_accuracy: 0.6620\n",
      "Epoch 81/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0044 - accuracy: 0.6982 - val_loss: 3.2863 - val_accuracy: 0.6620\n",
      "Epoch 82/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0039 - accuracy: 0.6978 - val_loss: 3.2999 - val_accuracy: 0.6613\n",
      "Epoch 83/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0020 - accuracy: 0.6985 - val_loss: 3.2944 - val_accuracy: 0.6640\n",
      "Epoch 84/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0029 - accuracy: 0.6983 - val_loss: 3.2952 - val_accuracy: 0.6660\n",
      "Epoch 85/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0040 - accuracy: 0.6970 - val_loss: 3.2827 - val_accuracy: 0.6680\n",
      "Epoch 86/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0036 - accuracy: 0.6984 - val_loss: 3.3202 - val_accuracy: 0.6487\n",
      "Epoch 87/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0007 - accuracy: 0.6973 - val_loss: 3.3026 - val_accuracy: 0.6533\n",
      "Epoch 88/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0031 - accuracy: 0.6981 - val_loss: 3.2793 - val_accuracy: 0.6613\n",
      "Epoch 89/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0032 - accuracy: 0.6981 - val_loss: 3.3077 - val_accuracy: 0.6453\n",
      "Epoch 90/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0007 - accuracy: 0.6996 - val_loss: 3.3165 - val_accuracy: 0.6540\n",
      "Epoch 91/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 3.0011 - accuracy: 0.6983 - val_loss: 3.3106 - val_accuracy: 0.6667\n",
      "Epoch 92/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9976 - accuracy: 0.6991 - val_loss: 3.2833 - val_accuracy: 0.6633\n",
      "Epoch 93/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9955 - accuracy: 0.6993 - val_loss: 3.2954 - val_accuracy: 0.6633\n",
      "Epoch 94/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9949 - accuracy: 0.6984 - val_loss: 3.2983 - val_accuracy: 0.6607\n",
      "Epoch 95/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9983 - accuracy: 0.6986 - val_loss: 3.2960 - val_accuracy: 0.6600\n",
      "Epoch 96/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9996 - accuracy: 0.6994 - val_loss: 3.2845 - val_accuracy: 0.6673\n",
      "Epoch 97/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9936 - accuracy: 0.6985 - val_loss: 3.2993 - val_accuracy: 0.6607\n",
      "Epoch 98/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9951 - accuracy: 0.6988 - val_loss: 3.3157 - val_accuracy: 0.6540\n",
      "Epoch 99/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9963 - accuracy: 0.6990 - val_loss: 3.2951 - val_accuracy: 0.6553\n",
      "Epoch 100/100\n",
      "1516/1516 [==============================] - 3s 2ms/step - loss: 2.9929 - accuracy: 0.6991 - val_loss: 3.2960 - val_accuracy: 0.6587\n"
     ]
    }
   ],
   "source": [
    "# Fine tuning the model\n",
    "\n",
    "# We also measure the time taken to compute\n",
    "t = time.perf_counter()\n",
    "history_tune = model.fit([X_1, X_2, X_3, X_4], y_data, batch_size=32, epochs=100, validation_split = 0.03)\n",
    "t_c = time.perf_counter() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we read the test data to evaluate our model\n",
    "\n",
    "\n",
    "# Reading the Test RBI data\n",
    "df_test = pd.read_csv(\"Test Data for RBI 0.88 mean 2000_itr(N=30)/Test_Data_rbi_vectors.csv\")\n",
    "#print(df_test)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e54ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "tem_test = df_test.to_numpy()\n",
    "#print(tem_test)\n",
    "tem_test.shape\n",
    "X_1_test = tem_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc551c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Test channel data\n",
    "df1_test = pd.read_csv(\"Test Data for RBI 0.88 mean 2000_itr(N=30)/Test_Data_channel_matrices.csv\")\n",
    "#print(df1_test)\n",
    "df1_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ce88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_2_test = df1_test.to_numpy()\n",
    "X_2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_test = X_2_test.reshape((itr_test,N,N))\n",
    "#print(X_2_test)\n",
    "X_2_test = np.nan_to_num(X_2_test)\n",
    "#X_2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Test Source_Destination_info data\n",
    "\n",
    "df2_test = pd.read_csv(\"Test Data for RBI 0.88 mean 2000_itr(N=30)/Test_Data_Source_Destination_info_matrix(only Src Des).csv\")\n",
    "#print(df2_test)\n",
    "df2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c06566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_3_test = df2_test.to_numpy()\n",
    "#print(X_3_test)\n",
    "X_3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3_test = X_3_test.astype(np.float64)\n",
    "print(X_3_test.dtype)\n",
    "#print(X_3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea766d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Test Spectrum_occupancy data, i.e, NSA\n",
    "\n",
    "df3_test = pd.read_csv(\"Test Data for RBI 0.88 mean 2000_itr(N=30)/Test_Data_spectrum_matrix.csv\")\n",
    "#print(df3_test)\n",
    "df3_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_4_test = df3_test.to_numpy()\n",
    "#print(X_4_test)\n",
    "X_4_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fe49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_4_test = X_4_test.astype(np.float64)\n",
    "print(X_4_test.dtype)\n",
    "#print(X_4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448aa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Test output power data\n",
    "\n",
    "df4_test = pd.read_csv(\"Test Data for RBI 0.88 mean 2000_itr(N=30)/Test_Data_output1.csv\")\n",
    "#print(df4_test)\n",
    "df4_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting it into numpy array\n",
    "X_5_test = df4_test.to_numpy()\n",
    "#print(X_5_test)\n",
    "print(X_5_test.shape)\n",
    "X_5_test = np.nan_to_num(X_5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell converts each of the (1,30) y_data_test output to (1,6)\n",
    "\n",
    "a=list(range(0, itr_test))\n",
    "max1 = 0\n",
    "count = 0\n",
    "maxlen = 0\n",
    "max_path = np.zeros(30)\n",
    "max_path_ind = 0\n",
    "X_5_test_new = np.zeros((itr_test,6))\n",
    "for i in a:\n",
    "    temp = X_5_test[i]\n",
    "    maxlen = len(temp[np.nonzero(temp)])\n",
    "    if maxlen > 6:\n",
    "        X_5_test_new[i,:] = temp[0:6]\n",
    "        temp_non_zero = temp[np.nonzero(temp)]\n",
    "        X_5_test_new[i,5] = temp_non_zero[-1]\n",
    "        continue\n",
    "    X_5_test_new[i,:] = temp[0:6]\n",
    "    #print(\"original\")\n",
    "    #print(X_5[i])\n",
    "X_5_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab61b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_test = X_5_test_new.astype(np.float64)\n",
    "print(y_data_test.dtype)\n",
    "#print(y_data_test)\n",
    "y_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_test.dtype, X_2_test.dtype, X_3_test.dtype, X_4_test.dtype, y_data_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_test.shape, X_2_test.shape, X_3_test.shape, X_4_test.shape, y_data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_test = [X_1_test, X_2_test, X_3_test, X_4_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y_data_test to tensor\n",
    "\n",
    "y_data_test = tf.constant(y_data_test)\n",
    "print(y_data_test.shape)\n",
    "#y_data_test = tf.reshape(y_data_test, [itr_test,1,1,N*N])\n",
    "#print(y_data_test.shape)\n",
    "y_data_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scale the non zero values of Test output power so that there is some sizable error if the model outputs a zero matrix\n",
    "#scaling_parameter = 100\n",
    "\n",
    "y_data_test = y_data_test*scaling_parameter\n",
    "y_data_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80677b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make predictions using our model\n",
    "import time\n",
    "t = time.perf_counter()\n",
    "y_pred = model.predict(X_data_test)\n",
    "elapsed = time.perf_counter() - t\n",
    "print(elapsed)\n",
    "print(y_pred.shape)\n",
    "print(\"For 1 prediction\")\n",
    "print(elapsed/itr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c730ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total time taken to train and predict one sample\n",
    "t_c = t_c + elapsed\n",
    "print(\"Total time taken to train and predict all sample\")\n",
    "print(t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "model.evaluate(X_data_test, y_data_test)\n",
    "elapsed1 = time.perf_counter() - t1\n",
    "print(elapsed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(1,(6)+1)\n",
    "x_axis.shape\n",
    "ind = 3\n",
    "y_pred = np.round(y_pred)\n",
    "print(y_pred[ind]/scaling_parameter)\n",
    "y_data_test[ind]/scaling_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc206e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(0, itr_test))\n",
    "#for i in a:\n",
    "#    plt.figure()\n",
    "#    plt.plot(x_axis,y_data_test[i]/scaling_parameter,'b',x_axis,y_pred[i]/scaling_parameter,'r')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.ylim((0.2, 1))\n",
    "plt.plot(history_tune.history['accuracy'])\n",
    "plt.plot(history_tune.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2be344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "#plt.ylim((20, 40))\n",
    "plt.plot(history_tune.history['loss'])\n",
    "plt.plot(history_tune.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/RBI_TLWFT_0.9_mean(1-12-2023)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
